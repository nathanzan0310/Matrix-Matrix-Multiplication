# APMA 2822B — HW2 Roofline Report

This assignment involved implementing and analyzing matrix–matrix multiplication (`C ← C + A·B`) using different loop orders and optimization strategies. Performance is summarized with a roofline model (GFLOP/s vs arithmetic intensity, AI), using 160 GB/s as the memory-bandwidth ceiling and 1 TFLOP/s as the compute ceiling.

Five implementations were tested: `ijk`, `jik`, `kij` (serial), a blocked (tiled) kernel, and an OpenMP version (`omp_ijk`) with 16 threads. Arithmetic intensity used the lower-bound traffic model `bytes ≈ (N·K + K·M + 2·N·M)·8` and FLOPs `= 2·N·M·K`. Each configuration was swept over multiple sizes to obtain time, GFLOP/s, and AI. Correctness was checked against a reference multiply for representative sizes.

The serial results (`roofline_serial.png`) show that all loop orders are far below both roofs, with best points around **6–8 GFLOP/s**. Ordering matters: **`kij` > `ijk` ≈ `jik`** across most sizes, consistent with better reuse of `B[k,*]` and more coherent accumulation into `C[i,*]` in `kij`. The OpenMP results with 16 threads (`roofline_omp16.png`) do **not** scale linearly; most points fall in the **~0.3–5 GFLOP/s** range with a few near **7–8 GFLOP/s**. The lack of strong speedup indicates the kernel remains **memory-bound**; naive parallelization increases memory traffic and contention without improving per-thread locality, and multi-threaded updates to `C` incur write-allocate and cache-line churn.

The blocking sweep (`roofline_blocked.png`) isolates the effect of cache tiling (built with `-O0`, per the assignment) and shows **sub-GFLOP to a few-GFLOP/s** results (≈**0.3–3 GFLOP/s** for most points) that remain well below the bandwidth roof. Within that low-optimization setting, the blocked points typically sit at or near the top of their local clusters and at slightly higher AI than the unblocked loop orders, which indicates **improved data reuse** from tiling; however, the absolute gains are modest. This is expected because (i) `-O0` suppresses vectorization and compiler loop transforms, (ii) tile sizes were not aggressively tuned to the cache hierarchy, and (iii) we did not apply register-blocking or a `B^T` layout to make the innermost loop fully contiguous. Even with blocking, the measurements confirm a **memory-traffic-limited** regime rather than a compute-limited one.

Overall, all three figures tell a consistent story: performance is dominated by memory behavior, not raw FLOPs. Loop reordering (`kij`) and blocking both help by improving locality (raising effective AI and reducing off-chip bytes), but neither shifts points close to the roofs on their own. OpenMP does not change the roofline position meaningfully without per-thread tiling and better write patterns. The clearest next steps to move points upward are: parallelizing **over tiles** (give each thread its own `C` tile and corresponding `A/B` tiles), **tuning BN/BM/BK** to fit L2/L3, adding **register blocking** and an innermost kernel that vectorizes cleanly (potentially after **transposing B**), and, for peak runs, compiling with **`-O3 -march=native`** (keeping `-O0` only for the “blocking effect” study). These changes increase reuse per byte, improve alignment with SIMD, and reduce write-allocate overhead—precisely what the roofline model prescribes for climbing toward the bandwidth roof and, eventually, the compute roof.
